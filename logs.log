2023-06-24 16:38:32,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 16:38:32,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 16:38:32,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 16:38:32,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 16:38:34,849:INFO:Soft dependency imported: prophet: 1.1.4
2023-06-24 16:39:15,483:INFO:PyCaret RegressionExperiment
2023-06-24 16:39:15,483:INFO:Logging name: reg-default-name
2023-06-24 16:39:15,483:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-24 16:39:15,483:INFO:version 3.0.2
2023-06-24 16:39:15,483:INFO:Initializing setup()
2023-06-24 16:39:15,483:INFO:self.USI: 6c5c
2023-06-24 16:39:15,484:INFO:self._variable_keys: {'y_test', 'html_param', '_ml_usecase', 'fold_generator', 'exp_name_log', 'X_test', 'data', 'exp_id', 'memory', 'fold_groups_param', 'pipeline', 'idx', 'fold_shuffle_param', 'transform_target_param', 'y', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', 'seed', 'gpu_param', 'X_train', 'n_jobs_param', 'y_train', 'USI', 'X', 'logging_param', '_available_plots'}
2023-06-24 16:39:15,484:INFO:Checking environment
2023-06-24 16:39:15,484:INFO:python_version: 3.10.12
2023-06-24 16:39:15,484:INFO:python_build: ('main', 'Jun  7 2023 12:45:35')
2023-06-24 16:39:15,484:INFO:machine: x86_64
2023-06-24 16:39:15,484:INFO:platform: Linux-5.15.107+-x86_64-with-glibc2.31
2023-06-24 16:39:15,484:INFO:Memory: svmem(total=13613326336, available=12089069568, percent=11.2, used=1198952448, free=7994638336, active=917520384, inactive=4372951040, buffers=74985472, cached=4344750080, shared=2338816, slab=192933888)
2023-06-24 16:39:15,485:INFO:Physical Core: 1
2023-06-24 16:39:15,485:INFO:Logical Core: 2
2023-06-24 16:39:15,485:INFO:Checking libraries
2023-06-24 16:39:15,485:INFO:System:
2023-06-24 16:39:15,485:INFO:    python: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]
2023-06-24 16:39:15,485:INFO:executable: /usr/bin/python3
2023-06-24 16:39:15,485:INFO:   machine: Linux-5.15.107+-x86_64-with-glibc2.31
2023-06-24 16:39:15,485:INFO:PyCaret required dependencies:
2023-06-24 16:39:15,485:INFO:                 pip: 23.1.2
2023-06-24 16:39:15,485:INFO:          setuptools: 67.7.2
2023-06-24 16:39:15,485:INFO:             pycaret: 3.0.2
2023-06-24 16:39:15,485:INFO:             IPython: 7.34.0
2023-06-24 16:39:15,485:INFO:          ipywidgets: 7.7.1
2023-06-24 16:39:15,486:INFO:                tqdm: 4.65.0
2023-06-24 16:39:15,486:INFO:               numpy: 1.22.4
2023-06-24 16:39:15,486:INFO:              pandas: 1.5.3
2023-06-24 16:39:15,486:INFO:              jinja2: 3.1.2
2023-06-24 16:39:15,486:INFO:               scipy: 1.10.1
2023-06-24 16:39:15,486:INFO:              joblib: 1.2.0
2023-06-24 16:39:15,486:INFO:             sklearn: 1.2.2
2023-06-24 16:39:15,486:INFO:                pyod: 1.0.9
2023-06-24 16:39:15,486:INFO:            imblearn: 0.10.1
2023-06-24 16:39:15,486:INFO:   category_encoders: 2.6.1
2023-06-24 16:39:15,486:INFO:            lightgbm: 3.3.5
2023-06-24 16:39:15,486:INFO:               numba: 0.56.4
2023-06-24 16:39:15,486:INFO:            requests: 2.27.1
2023-06-24 16:39:15,486:INFO:          matplotlib: 3.7.1
2023-06-24 16:39:15,486:INFO:          scikitplot: 0.3.7
2023-06-24 16:39:15,486:INFO:         yellowbrick: 1.5
2023-06-24 16:39:15,486:INFO:              plotly: 5.13.1
2023-06-24 16:39:15,486:INFO:             kaleido: 0.2.1
2023-06-24 16:39:15,486:INFO:         statsmodels: 0.13.5
2023-06-24 16:39:15,486:INFO:              sktime: 0.17.0
2023-06-24 16:39:15,486:INFO:               tbats: 1.1.3
2023-06-24 16:39:15,486:INFO:            pmdarima: 2.0.3
2023-06-24 16:39:15,486:INFO:              psutil: 5.9.5
2023-06-24 16:39:15,486:INFO:PyCaret optional dependencies:
2023-06-24 16:39:15,510:INFO:                shap: Not installed
2023-06-24 16:39:15,510:INFO:           interpret: Not installed
2023-06-24 16:39:15,510:INFO:                umap: Not installed
2023-06-24 16:39:15,510:INFO:    pandas_profiling: Not installed
2023-06-24 16:39:15,511:INFO:  explainerdashboard: Not installed
2023-06-24 16:39:15,511:INFO:             autoviz: Not installed
2023-06-24 16:39:15,511:INFO:           fairlearn: Not installed
2023-06-24 16:39:15,511:INFO:             xgboost: 1.7.6
2023-06-24 16:39:15,511:INFO:            catboost: Not installed
2023-06-24 16:39:15,511:INFO:              kmodes: Not installed
2023-06-24 16:39:15,511:INFO:             mlxtend: 0.14.0
2023-06-24 16:39:15,511:INFO:       statsforecast: Not installed
2023-06-24 16:39:15,511:INFO:        tune_sklearn: Not installed
2023-06-24 16:39:15,511:INFO:                 ray: Not installed
2023-06-24 16:39:15,511:INFO:            hyperopt: 0.2.7
2023-06-24 16:39:15,511:INFO:              optuna: Not installed
2023-06-24 16:39:15,511:INFO:               skopt: Not installed
2023-06-24 16:39:15,511:INFO:              mlflow: Not installed
2023-06-24 16:39:15,511:INFO:              gradio: Not installed
2023-06-24 16:39:15,511:INFO:             fastapi: Not installed
2023-06-24 16:39:15,511:INFO:             uvicorn: Not installed
2023-06-24 16:39:15,511:INFO:              m2cgen: Not installed
2023-06-24 16:39:15,511:INFO:           evidently: Not installed
2023-06-24 16:39:15,511:INFO:               fugue: Not installed
2023-06-24 16:39:15,511:INFO:           streamlit: Not installed
2023-06-24 16:39:15,511:INFO:             prophet: 1.1.4
2023-06-24 16:39:15,511:INFO:None
2023-06-24 16:39:15,511:INFO:Set up data.
2023-06-24 16:39:15,521:INFO:Set up train/test split.
2023-06-24 16:39:15,529:INFO:Set up index.
2023-06-24 16:39:15,529:INFO:Set up folding strategy.
2023-06-24 16:39:15,529:INFO:Assigning column types.
2023-06-24 16:39:15,534:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-24 16:39:15,535:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-24 16:39:15,540:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:15,546:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:15,612:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:15,663:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:15,664:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:16,234:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:16,235:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,240:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,245:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,313:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,363:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,365:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:16,368:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:16,370:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-24 16:39:16,377:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,383:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,450:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,498:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,500:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:16,503:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:16,509:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,514:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,578:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,652:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,654:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:16,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:16,660:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-24 16:39:16,680:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,811:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,923:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:16,924:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:16,930:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:16,968:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,111:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,204:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,206:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:17,211:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:17,212:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-24 16:39:17,406:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,505:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,506:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:17,520:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:17,695:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,828:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:17,830:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:17,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:17,842:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-24 16:39:18,045:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:18,146:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:18,152:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:18,310:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:18,438:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:18,444:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:18,444:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-24 16:39:18,698:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:18,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:19,063:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:19,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:19,071:INFO:Preparing preprocessing pipeline...
2023-06-24 16:39:19,071:INFO:Set up simple imputation.
2023-06-24 16:39:19,079:INFO:Set up encoding of ordinal features.
2023-06-24 16:39:19,084:INFO:Set up encoding of categorical features.
2023-06-24 16:39:19,084:INFO:Set up feature normalization.
2023-06-24 16:39:19,384:INFO:Finished creating preprocessing pipeline.
2023-06-24 16:39:19,522:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64},
                                                                        {'col': 'smoker',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-24 16:39:19,522:INFO:Creating final display dataframe.
2023-06-24 16:39:20,225:INFO:Setup _display_container:                     Description             Value
0                    Session id              7402
1                        Target           charges
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 10)
5   Transformed train set shape        (1070, 10)
6    Transformed test set shape         (268, 10)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16                    Normalize              True
17             Normalize method            zscore
18               Fold Generator             KFold
19                  Fold Number                10
20                     CPU Jobs                -1
21                      Use GPU             False
22               Log Experiment             False
23              Experiment Name  reg-default-name
24                          USI              6c5c
2023-06-24 16:39:20,657:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:20,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:20,920:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:20,925:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:20,926:INFO:setup() successfully completed in 5.45s...............
2023-06-24 16:39:24,740:INFO:PyCaret RegressionExperiment
2023-06-24 16:39:24,741:INFO:Logging name: reg-default-name
2023-06-24 16:39:24,742:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-24 16:39:24,742:INFO:version 3.0.2
2023-06-24 16:39:24,742:INFO:Initializing setup()
2023-06-24 16:39:24,742:INFO:self.USI: a52d
2023-06-24 16:39:24,742:INFO:self._variable_keys: {'y_test', 'html_param', '_ml_usecase', 'fold_generator', 'exp_name_log', 'X_test', 'data', 'exp_id', 'memory', 'fold_groups_param', 'pipeline', 'idx', 'fold_shuffle_param', 'transform_target_param', 'y', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', 'seed', 'gpu_param', 'X_train', 'n_jobs_param', 'y_train', 'USI', 'X', 'logging_param', '_available_plots'}
2023-06-24 16:39:24,742:INFO:Checking environment
2023-06-24 16:39:24,742:INFO:python_version: 3.10.12
2023-06-24 16:39:24,742:INFO:python_build: ('main', 'Jun  7 2023 12:45:35')
2023-06-24 16:39:24,742:INFO:machine: x86_64
2023-06-24 16:39:24,742:INFO:platform: Linux-5.15.107+-x86_64-with-glibc2.31
2023-06-24 16:39:24,742:INFO:Memory: svmem(total=13613326336, available=11892690944, percent=12.6, used=1394749440, free=7748771840, active=918351872, inactive=4627800064, buffers=75165696, cached=4394639360, shared=2338816, slab=194359296)
2023-06-24 16:39:24,743:INFO:Physical Core: 1
2023-06-24 16:39:24,743:INFO:Logical Core: 2
2023-06-24 16:39:24,743:INFO:Checking libraries
2023-06-24 16:39:24,743:INFO:System:
2023-06-24 16:39:24,743:INFO:    python: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]
2023-06-24 16:39:24,743:INFO:executable: /usr/bin/python3
2023-06-24 16:39:24,743:INFO:   machine: Linux-5.15.107+-x86_64-with-glibc2.31
2023-06-24 16:39:24,743:INFO:PyCaret required dependencies:
2023-06-24 16:39:24,743:INFO:                 pip: 23.1.2
2023-06-24 16:39:24,743:INFO:          setuptools: 67.7.2
2023-06-24 16:39:24,743:INFO:             pycaret: 3.0.2
2023-06-24 16:39:24,743:INFO:             IPython: 7.34.0
2023-06-24 16:39:24,743:INFO:          ipywidgets: 7.7.1
2023-06-24 16:39:24,743:INFO:                tqdm: 4.65.0
2023-06-24 16:39:24,743:INFO:               numpy: 1.22.4
2023-06-24 16:39:24,743:INFO:              pandas: 1.5.3
2023-06-24 16:39:24,743:INFO:              jinja2: 3.1.2
2023-06-24 16:39:24,743:INFO:               scipy: 1.10.1
2023-06-24 16:39:24,743:INFO:              joblib: 1.2.0
2023-06-24 16:39:24,743:INFO:             sklearn: 1.2.2
2023-06-24 16:39:24,743:INFO:                pyod: 1.0.9
2023-06-24 16:39:24,743:INFO:            imblearn: 0.10.1
2023-06-24 16:39:24,744:INFO:   category_encoders: 2.6.1
2023-06-24 16:39:24,744:INFO:            lightgbm: 3.3.5
2023-06-24 16:39:24,744:INFO:               numba: 0.56.4
2023-06-24 16:39:24,744:INFO:            requests: 2.27.1
2023-06-24 16:39:24,744:INFO:          matplotlib: 3.7.1
2023-06-24 16:39:24,744:INFO:          scikitplot: 0.3.7
2023-06-24 16:39:24,744:INFO:         yellowbrick: 1.5
2023-06-24 16:39:24,744:INFO:              plotly: 5.13.1
2023-06-24 16:39:24,744:INFO:             kaleido: 0.2.1
2023-06-24 16:39:24,744:INFO:         statsmodels: 0.13.5
2023-06-24 16:39:24,744:INFO:              sktime: 0.17.0
2023-06-24 16:39:24,747:INFO:               tbats: 1.1.3
2023-06-24 16:39:24,747:INFO:            pmdarima: 2.0.3
2023-06-24 16:39:24,747:INFO:              psutil: 5.9.5
2023-06-24 16:39:24,747:INFO:PyCaret optional dependencies:
2023-06-24 16:39:24,747:INFO:                shap: Not installed
2023-06-24 16:39:24,747:INFO:           interpret: Not installed
2023-06-24 16:39:24,747:INFO:                umap: Not installed
2023-06-24 16:39:24,747:INFO:    pandas_profiling: Not installed
2023-06-24 16:39:24,747:INFO:  explainerdashboard: Not installed
2023-06-24 16:39:24,748:INFO:             autoviz: Not installed
2023-06-24 16:39:24,748:INFO:           fairlearn: Not installed
2023-06-24 16:39:24,748:INFO:             xgboost: 1.7.6
2023-06-24 16:39:24,748:INFO:            catboost: Not installed
2023-06-24 16:39:24,748:INFO:              kmodes: Not installed
2023-06-24 16:39:24,748:INFO:             mlxtend: 0.14.0
2023-06-24 16:39:24,748:INFO:       statsforecast: Not installed
2023-06-24 16:39:24,748:INFO:        tune_sklearn: Not installed
2023-06-24 16:39:24,748:INFO:                 ray: Not installed
2023-06-24 16:39:24,748:INFO:            hyperopt: 0.2.7
2023-06-24 16:39:24,748:INFO:              optuna: Not installed
2023-06-24 16:39:24,748:INFO:               skopt: Not installed
2023-06-24 16:39:24,748:INFO:              mlflow: Not installed
2023-06-24 16:39:24,748:INFO:              gradio: Not installed
2023-06-24 16:39:24,748:INFO:             fastapi: Not installed
2023-06-24 16:39:24,748:INFO:             uvicorn: Not installed
2023-06-24 16:39:24,748:INFO:              m2cgen: Not installed
2023-06-24 16:39:24,748:INFO:           evidently: Not installed
2023-06-24 16:39:24,748:INFO:               fugue: Not installed
2023-06-24 16:39:24,748:INFO:           streamlit: Not installed
2023-06-24 16:39:24,748:INFO:             prophet: 1.1.4
2023-06-24 16:39:24,748:INFO:None
2023-06-24 16:39:24,748:INFO:Set up data.
2023-06-24 16:39:24,784:INFO:Set up train/test split.
2023-06-24 16:39:24,825:INFO:Set up index.
2023-06-24 16:39:24,826:INFO:Set up folding strategy.
2023-06-24 16:39:24,826:INFO:Assigning column types.
2023-06-24 16:39:24,901:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-24 16:39:24,902:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-24 16:39:24,934:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:24,951:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:25,398:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:25,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:25,722:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:25,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:25,757:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-24 16:39:25,810:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:25,860:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:26,438:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:26,660:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:26,668:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:26,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:26,677:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-24 16:39:26,696:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:26,715:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,069:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,302:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,303:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:27,323:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:27,347:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,688:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,922:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:27,934:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:27,952:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:27,953:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-24 16:39:27,992:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:28,280:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:28,485:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:28,489:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:28,517:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:28,537:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-24 16:39:28,705:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:28,873:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:28,875:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:28,880:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:28,881:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-24 16:39:29,089:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:29,199:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:29,200:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:29,206:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:29,352:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:29,477:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-24 16:39:29,478:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:29,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:29,485:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-24 16:39:29,773:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:29,989:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:29,994:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:30,146:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-24 16:39:30,248:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:30,254:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:30,255:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-24 16:39:30,574:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:30,580:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:30,869:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:30,875:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:30,876:INFO:Preparing preprocessing pipeline...
2023-06-24 16:39:30,876:INFO:Set up simple imputation.
2023-06-24 16:39:30,883:INFO:Set up encoding of ordinal features.
2023-06-24 16:39:30,889:INFO:Set up encoding of categorical features.
2023-06-24 16:39:30,890:INFO:Set up polynomial features.
2023-06-24 16:39:30,890:INFO:Set up binning of numerical features.
2023-06-24 16:39:30,892:INFO:Set up feature normalization.
2023-06-24 16:39:31,462:INFO:Finished creating preprocessing pipeline.
2023-06-24 16:39:31,557:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-24 16:39:31,557:INFO:Creating final display dataframe.
2023-06-24 16:39:32,352:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target           charges
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 55)
5   Transformed train set shape         (936, 55)
6    Transformed test set shape         (402, 55)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16          Polynomial features              True
17            Polynomial degree                 2
18                    Normalize              True
19             Normalize method            zscore
20               Fold Generator             KFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  reg-default-name
26                          USI              a52d
2023-06-24 16:39:32,621:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:32,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:32,898:INFO:Soft dependency imported: xgboost: 1.7.6
2023-06-24 16:39:32,904:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-24 16:39:32,905:INFO:setup() successfully completed in 8.17s...............
2023-06-24 16:39:36,081:INFO:Initializing compare_models()
2023-06-24 16:39:36,082:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-06-24 16:39:36,082:INFO:Checking exceptions
2023-06-24 16:39:36,086:INFO:Preparing display monitor
2023-06-24 16:39:36,141:INFO:Initializing Linear Regression
2023-06-24 16:39:36,141:INFO:Total runtime is 3.814697265625e-06 minutes
2023-06-24 16:39:36,149:INFO:SubProcess create_model() called ==================================
2023-06-24 16:39:36,150:INFO:Initializing create_model()
2023-06-24 16:39:36,150:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:39:36,150:INFO:Checking exceptions
2023-06-24 16:39:36,150:INFO:Importing libraries
2023-06-24 16:39:36,150:INFO:Copying training dataset
2023-06-24 16:39:36,159:INFO:Defining folds
2023-06-24 16:39:36,159:INFO:Declaring metric variables
2023-06-24 16:39:36,167:INFO:Importing untrained model
2023-06-24 16:39:36,175:INFO:Linear Regression Imported successfully
2023-06-24 16:39:36,191:INFO:Starting cross validation
2023-06-24 16:39:36,206:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:39:44,821:INFO:Calculating mean and std
2023-06-24 16:39:44,823:INFO:Creating metrics dataframe
2023-06-24 16:39:44,836:INFO:Uploading results into container
2023-06-24 16:39:44,838:INFO:Uploading model into container now
2023-06-24 16:39:44,838:INFO:_master_model_container: 1
2023-06-24 16:39:44,839:INFO:_display_container: 2
2023-06-24 16:39:44,839:INFO:LinearRegression(n_jobs=-1)
2023-06-24 16:39:44,839:INFO:create_model() successfully completed......................................
2023-06-24 16:39:45,082:INFO:SubProcess create_model() end ==================================
2023-06-24 16:39:45,082:INFO:Creating metrics dataframe
2023-06-24 16:39:45,094:INFO:Initializing Lasso Regression
2023-06-24 16:39:45,095:INFO:Total runtime is 0.14922244548797609 minutes
2023-06-24 16:39:45,102:INFO:SubProcess create_model() called ==================================
2023-06-24 16:39:45,103:INFO:Initializing create_model()
2023-06-24 16:39:45,103:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:39:45,103:INFO:Checking exceptions
2023-06-24 16:39:45,103:INFO:Importing libraries
2023-06-24 16:39:45,103:INFO:Copying training dataset
2023-06-24 16:39:45,113:INFO:Defining folds
2023-06-24 16:39:45,114:INFO:Declaring metric variables
2023-06-24 16:39:45,122:INFO:Importing untrained model
2023-06-24 16:39:45,130:INFO:Lasso Regression Imported successfully
2023-06-24 16:39:45,145:INFO:Starting cross validation
2023-06-24 16:39:45,148:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:39:45,452:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+09, tolerance: 1.228e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:45,503:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+09, tolerance: 1.224e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:46,022:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.418e+09, tolerance: 1.305e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:46,110:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e+09, tolerance: 1.271e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:46,614:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.041e+09, tolerance: 1.260e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:46,670:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.122e+09, tolerance: 1.240e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:47,199:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+09, tolerance: 1.250e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:47,254:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.099e+09, tolerance: 1.215e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:47,749:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.336e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:47,828:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.348e+09, tolerance: 1.242e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-24 16:39:48,065:INFO:Calculating mean and std
2023-06-24 16:39:48,067:INFO:Creating metrics dataframe
2023-06-24 16:39:48,081:INFO:Uploading results into container
2023-06-24 16:39:48,082:INFO:Uploading model into container now
2023-06-24 16:39:48,083:INFO:_master_model_container: 2
2023-06-24 16:39:48,083:INFO:_display_container: 2
2023-06-24 16:39:48,083:INFO:Lasso(random_state=123)
2023-06-24 16:39:48,083:INFO:create_model() successfully completed......................................
2023-06-24 16:39:48,309:INFO:SubProcess create_model() end ==================================
2023-06-24 16:39:48,309:INFO:Creating metrics dataframe
2023-06-24 16:39:48,321:INFO:Initializing Ridge Regression
2023-06-24 16:39:48,322:INFO:Total runtime is 0.20300562779108683 minutes
2023-06-24 16:39:48,331:INFO:SubProcess create_model() called ==================================
2023-06-24 16:39:48,331:INFO:Initializing create_model()
2023-06-24 16:39:48,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:39:48,331:INFO:Checking exceptions
2023-06-24 16:39:48,331:INFO:Importing libraries
2023-06-24 16:39:48,331:INFO:Copying training dataset
2023-06-24 16:39:48,339:INFO:Defining folds
2023-06-24 16:39:48,339:INFO:Declaring metric variables
2023-06-24 16:39:48,352:INFO:Importing untrained model
2023-06-24 16:39:48,363:INFO:Ridge Regression Imported successfully
2023-06-24 16:39:48,377:INFO:Starting cross validation
2023-06-24 16:39:48,381:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:39:50,998:INFO:Calculating mean and std
2023-06-24 16:39:51,003:INFO:Creating metrics dataframe
2023-06-24 16:39:51,019:INFO:Uploading results into container
2023-06-24 16:39:51,020:INFO:Uploading model into container now
2023-06-24 16:39:51,021:INFO:_master_model_container: 3
2023-06-24 16:39:51,021:INFO:_display_container: 2
2023-06-24 16:39:51,021:INFO:Ridge(random_state=123)
2023-06-24 16:39:51,021:INFO:create_model() successfully completed......................................
2023-06-24 16:39:51,209:INFO:SubProcess create_model() end ==================================
2023-06-24 16:39:51,209:INFO:Creating metrics dataframe
2023-06-24 16:39:51,222:INFO:Initializing Elastic Net
2023-06-24 16:39:51,223:INFO:Total runtime is 0.25135879119237264 minutes
2023-06-24 16:39:51,230:INFO:SubProcess create_model() called ==================================
2023-06-24 16:39:51,231:INFO:Initializing create_model()
2023-06-24 16:39:51,231:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:39:51,231:INFO:Checking exceptions
2023-06-24 16:39:51,231:INFO:Importing libraries
2023-06-24 16:39:51,231:INFO:Copying training dataset
2023-06-24 16:39:51,242:INFO:Defining folds
2023-06-24 16:39:51,242:INFO:Declaring metric variables
2023-06-24 16:39:51,250:INFO:Importing untrained model
2023-06-24 16:39:51,257:INFO:Elastic Net Imported successfully
2023-06-24 16:39:51,271:INFO:Starting cross validation
2023-06-24 16:39:51,278:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:39:54,052:INFO:Calculating mean and std
2023-06-24 16:39:54,055:INFO:Creating metrics dataframe
2023-06-24 16:39:54,073:INFO:Uploading results into container
2023-06-24 16:39:54,074:INFO:Uploading model into container now
2023-06-24 16:39:54,075:INFO:_master_model_container: 4
2023-06-24 16:39:54,075:INFO:_display_container: 2
2023-06-24 16:39:54,075:INFO:ElasticNet(random_state=123)
2023-06-24 16:39:54,075:INFO:create_model() successfully completed......................................
2023-06-24 16:39:54,307:INFO:SubProcess create_model() end ==================================
2023-06-24 16:39:54,307:INFO:Creating metrics dataframe
2023-06-24 16:39:54,327:INFO:Initializing Least Angle Regression
2023-06-24 16:39:54,327:INFO:Total runtime is 0.30309425592422484 minutes
2023-06-24 16:39:54,334:INFO:SubProcess create_model() called ==================================
2023-06-24 16:39:54,334:INFO:Initializing create_model()
2023-06-24 16:39:54,334:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:39:54,334:INFO:Checking exceptions
2023-06-24 16:39:54,335:INFO:Importing libraries
2023-06-24 16:39:54,335:INFO:Copying training dataset
2023-06-24 16:39:54,345:INFO:Defining folds
2023-06-24 16:39:54,346:INFO:Declaring metric variables
2023-06-24 16:39:54,352:INFO:Importing untrained model
2023-06-24 16:39:54,359:INFO:Least Angle Regression Imported successfully
2023-06-24 16:39:54,372:INFO:Starting cross validation
2023-06-24 16:39:54,375:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:39:54,930:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.168e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,932:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.068e+02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,935:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.647e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,941:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.377e+02, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,942:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.203e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,943:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.071e+02, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,943:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=9.850e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,945:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.710e+01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,945:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.663e+01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,945:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.381e+01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,945:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.022e+01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,954:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.367e+01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,956:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.598e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,956:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.268e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,957:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.578e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,957:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.303e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,957:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=9.262e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,962:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.761e+02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,980:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.095e+02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,990:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=3.798e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,991:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.471e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,991:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.988e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,998:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=3.048e+03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:54,998:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.030e+03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,962:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.642e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.938e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.923e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.739e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.695e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,980:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.876e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,982:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=8.267e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,983:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=6.897e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,983:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.594e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,984:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.978e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,984:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.455e+00, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,985:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=7.473e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,978:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.707e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,992:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.385e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,992:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.362e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:55,999:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.349e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,000:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.160e+01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,002:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=7.600e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,003:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=4.586e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,003:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=4.272e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,003:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.764e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,961:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.406e+02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,975:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.331e+02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,979:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=3.443e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,980:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.984e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,980:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.296e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,980:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.163e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:56,981:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.844e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:57,099:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.420e+03, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:57,101:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=3.217e+03, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:57,102:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=8.986e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:57,957:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.541e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:57,958:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.871e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:57,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=5.220e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,177:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.245e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,181:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.555e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,194:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.080e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,194:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.081e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,195:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.657e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,195:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.314e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,960:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.677e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,961:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.005e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,961:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=7.447e+01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,962:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.285e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,962:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.695e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,963:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.138e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:58,963:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.553e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,254:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.910e+02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,260:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=8.177e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,262:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=6.892e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,270:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.839e+03, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,397:INFO:Calculating mean and std
2023-06-24 16:39:59,402:INFO:Creating metrics dataframe
2023-06-24 16:39:59,421:INFO:Uploading results into container
2023-06-24 16:39:59,422:INFO:Uploading model into container now
2023-06-24 16:39:59,422:INFO:_master_model_container: 5
2023-06-24 16:39:59,423:INFO:_display_container: 2
2023-06-24 16:39:59,423:INFO:Lars(random_state=123)
2023-06-24 16:39:59,423:INFO:create_model() successfully completed......................................
2023-06-24 16:39:59,613:INFO:SubProcess create_model() end ==================================
2023-06-24 16:39:59,613:INFO:Creating metrics dataframe
2023-06-24 16:39:59,629:INFO:Initializing Lasso Least Angle Regression
2023-06-24 16:39:59,629:INFO:Total runtime is 0.39146585464477535 minutes
2023-06-24 16:39:59,637:INFO:SubProcess create_model() called ==================================
2023-06-24 16:39:59,638:INFO:Initializing create_model()
2023-06-24 16:39:59,638:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:39:59,638:INFO:Checking exceptions
2023-06-24 16:39:59,638:INFO:Importing libraries
2023-06-24 16:39:59,638:INFO:Copying training dataset
2023-06-24 16:39:59,647:INFO:Defining folds
2023-06-24 16:39:59,648:INFO:Declaring metric variables
2023-06-24 16:39:59,657:INFO:Importing untrained model
2023-06-24 16:39:59,666:INFO:Lasso Least Angle Regression Imported successfully
2023-06-24 16:39:59,682:INFO:Starting cross validation
2023-06-24 16:39:59,686:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:39:59,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.484e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,967:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.417e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,972:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=8.466e+01, previous alpha=8.466e+01, with an active set of 17 regressors.
  warnings.warn(

2023-06-24 16:39:59,984:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.057e+02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,985:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.890e+02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:39:59,989:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=1.272e+02, previous alpha=1.143e+02, with an active set of 14 regressors.
  warnings.warn(

2023-06-24 16:40:00,520:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.937e+02, previous alpha=2.567e+02, with an active set of 11 regressors.
  warnings.warn(

2023-06-24 16:40:00,557:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=1.279e+02, previous alpha=1.279e+02, with an active set of 13 regressors.
  warnings.warn(

2023-06-24 16:40:01,037:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=9.226e+01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,044:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.346e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,048:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 50 iterations, alpha=1.173e+01, previous alpha=1.173e+01, with an active set of 35 regressors.
  warnings.warn(

2023-06-24 16:40:01,126:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.859e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,127:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.700e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,136:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=3.844e+01, previous alpha=3.836e+01, with an active set of 25 regressors.
  warnings.warn(

2023-06-24 16:40:01,614:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.461e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,615:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.353e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,616:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.528e+01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,618:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=6.363e+01, previous alpha=5.973e+01, with an active set of 19 regressors.
  warnings.warn(

2023-06-24 16:40:01,736:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.305e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,739:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=6.011e+01, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,742:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.490e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,745:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.379e+01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:01,747:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 45 iterations, alpha=2.174e+01, previous alpha=2.154e+01, with an active set of 30 regressors.
  warnings.warn(

2023-06-24 16:40:02,155:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.517e+02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:02,155:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.342e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:02,156:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.285e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:02,157:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.257e+02, previous alpha=1.193e+02, with an active set of 13 regressors.
  warnings.warn(

2023-06-24 16:40:02,290:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.910e+02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:02,292:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.522e+02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:02,295:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.857e+01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-24 16:40:02,296:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=6.551e+01, previous alpha=6.536e+01, with an active set of 18 regressors.
  warnings.warn(

2023-06-24 16:40:02,507:INFO:Calculating mean and std
2023-06-24 16:40:02,516:INFO:Creating metrics dataframe
2023-06-24 16:40:02,534:INFO:Uploading results into container
2023-06-24 16:40:02,535:INFO:Uploading model into container now
2023-06-24 16:40:02,536:INFO:_master_model_container: 6
2023-06-24 16:40:02,536:INFO:_display_container: 2
2023-06-24 16:40:02,537:INFO:LassoLars(random_state=123)
2023-06-24 16:40:02,537:INFO:create_model() successfully completed......................................
2023-06-24 16:40:02,722:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:02,722:INFO:Creating metrics dataframe
2023-06-24 16:40:02,736:INFO:Initializing Orthogonal Matching Pursuit
2023-06-24 16:40:02,737:INFO:Total runtime is 0.4432612339655558 minutes
2023-06-24 16:40:02,745:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:02,746:INFO:Initializing create_model()
2023-06-24 16:40:02,746:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:02,746:INFO:Checking exceptions
2023-06-24 16:40:02,746:INFO:Importing libraries
2023-06-24 16:40:02,746:INFO:Copying training dataset
2023-06-24 16:40:02,756:INFO:Defining folds
2023-06-24 16:40:02,757:INFO:Declaring metric variables
2023-06-24 16:40:02,766:INFO:Importing untrained model
2023-06-24 16:40:02,774:INFO:Orthogonal Matching Pursuit Imported successfully
2023-06-24 16:40:02,789:INFO:Starting cross validation
2023-06-24 16:40:02,793:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:06,973:INFO:Calculating mean and std
2023-06-24 16:40:06,989:INFO:Creating metrics dataframe
2023-06-24 16:40:06,998:INFO:Uploading results into container
2023-06-24 16:40:06,999:INFO:Uploading model into container now
2023-06-24 16:40:07,000:INFO:_master_model_container: 7
2023-06-24 16:40:07,000:INFO:_display_container: 2
2023-06-24 16:40:07,000:INFO:OrthogonalMatchingPursuit()
2023-06-24 16:40:07,000:INFO:create_model() successfully completed......................................
2023-06-24 16:40:07,186:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:07,187:INFO:Creating metrics dataframe
2023-06-24 16:40:07,202:INFO:Initializing Bayesian Ridge
2023-06-24 16:40:07,203:INFO:Total runtime is 0.5176881432533263 minutes
2023-06-24 16:40:07,210:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:07,211:INFO:Initializing create_model()
2023-06-24 16:40:07,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:07,211:INFO:Checking exceptions
2023-06-24 16:40:07,211:INFO:Importing libraries
2023-06-24 16:40:07,211:INFO:Copying training dataset
2023-06-24 16:40:07,221:INFO:Defining folds
2023-06-24 16:40:07,221:INFO:Declaring metric variables
2023-06-24 16:40:07,228:INFO:Importing untrained model
2023-06-24 16:40:07,236:INFO:Bayesian Ridge Imported successfully
2023-06-24 16:40:07,253:INFO:Starting cross validation
2023-06-24 16:40:07,256:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:10,561:INFO:Calculating mean and std
2023-06-24 16:40:10,569:INFO:Creating metrics dataframe
2023-06-24 16:40:10,604:INFO:Uploading results into container
2023-06-24 16:40:10,605:INFO:Uploading model into container now
2023-06-24 16:40:10,606:INFO:_master_model_container: 8
2023-06-24 16:40:10,606:INFO:_display_container: 2
2023-06-24 16:40:10,607:INFO:BayesianRidge()
2023-06-24 16:40:10,607:INFO:create_model() successfully completed......................................
2023-06-24 16:40:10,844:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:10,844:INFO:Creating metrics dataframe
2023-06-24 16:40:10,867:INFO:Initializing Passive Aggressive Regressor
2023-06-24 16:40:10,870:INFO:Total runtime is 0.5788120150566101 minutes
2023-06-24 16:40:10,880:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:10,881:INFO:Initializing create_model()
2023-06-24 16:40:10,881:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:10,881:INFO:Checking exceptions
2023-06-24 16:40:10,882:INFO:Importing libraries
2023-06-24 16:40:10,882:INFO:Copying training dataset
2023-06-24 16:40:10,896:INFO:Defining folds
2023-06-24 16:40:10,897:INFO:Declaring metric variables
2023-06-24 16:40:10,905:INFO:Importing untrained model
2023-06-24 16:40:10,913:INFO:Passive Aggressive Regressor Imported successfully
2023-06-24 16:40:10,928:INFO:Starting cross validation
2023-06-24 16:40:10,932:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:17,090:INFO:Calculating mean and std
2023-06-24 16:40:17,097:INFO:Creating metrics dataframe
2023-06-24 16:40:17,124:INFO:Uploading results into container
2023-06-24 16:40:17,126:INFO:Uploading model into container now
2023-06-24 16:40:17,126:INFO:_master_model_container: 9
2023-06-24 16:40:17,126:INFO:_display_container: 2
2023-06-24 16:40:17,127:INFO:PassiveAggressiveRegressor(random_state=123)
2023-06-24 16:40:17,127:INFO:create_model() successfully completed......................................
2023-06-24 16:40:17,314:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:17,314:INFO:Creating metrics dataframe
2023-06-24 16:40:17,333:INFO:Initializing Huber Regressor
2023-06-24 16:40:17,333:INFO:Total runtime is 0.6865363399187723 minutes
2023-06-24 16:40:17,342:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:17,343:INFO:Initializing create_model()
2023-06-24 16:40:17,343:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:17,343:INFO:Checking exceptions
2023-06-24 16:40:17,344:INFO:Importing libraries
2023-06-24 16:40:17,344:INFO:Copying training dataset
2023-06-24 16:40:17,360:INFO:Defining folds
2023-06-24 16:40:17,360:INFO:Declaring metric variables
2023-06-24 16:40:17,368:INFO:Importing untrained model
2023-06-24 16:40:17,375:INFO:Huber Regressor Imported successfully
2023-06-24 16:40:17,389:INFO:Starting cross validation
2023-06-24 16:40:17,392:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:17,765:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:17,782:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:18,390:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:18,542:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:19,018:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:19,210:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:19,663:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:19,938:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:20,408:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:20,595:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-24 16:40:20,791:INFO:Calculating mean and std
2023-06-24 16:40:20,803:INFO:Creating metrics dataframe
2023-06-24 16:40:20,831:INFO:Uploading results into container
2023-06-24 16:40:20,833:INFO:Uploading model into container now
2023-06-24 16:40:20,833:INFO:_master_model_container: 10
2023-06-24 16:40:20,833:INFO:_display_container: 2
2023-06-24 16:40:20,834:INFO:HuberRegressor()
2023-06-24 16:40:20,834:INFO:create_model() successfully completed......................................
2023-06-24 16:40:21,015:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:21,016:INFO:Creating metrics dataframe
2023-06-24 16:40:21,033:INFO:Initializing K Neighbors Regressor
2023-06-24 16:40:21,033:INFO:Total runtime is 0.7481963555018106 minutes
2023-06-24 16:40:21,043:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:21,044:INFO:Initializing create_model()
2023-06-24 16:40:21,044:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:21,044:INFO:Checking exceptions
2023-06-24 16:40:21,044:INFO:Importing libraries
2023-06-24 16:40:21,044:INFO:Copying training dataset
2023-06-24 16:40:21,054:INFO:Defining folds
2023-06-24 16:40:21,055:INFO:Declaring metric variables
2023-06-24 16:40:21,062:INFO:Importing untrained model
2023-06-24 16:40:21,070:INFO:K Neighbors Regressor Imported successfully
2023-06-24 16:40:21,084:INFO:Starting cross validation
2023-06-24 16:40:21,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:23,864:INFO:Calculating mean and std
2023-06-24 16:40:23,878:INFO:Creating metrics dataframe
2023-06-24 16:40:23,909:INFO:Uploading results into container
2023-06-24 16:40:23,911:INFO:Uploading model into container now
2023-06-24 16:40:23,911:INFO:_master_model_container: 11
2023-06-24 16:40:23,911:INFO:_display_container: 2
2023-06-24 16:40:23,912:INFO:KNeighborsRegressor(n_jobs=-1)
2023-06-24 16:40:23,912:INFO:create_model() successfully completed......................................
2023-06-24 16:40:24,102:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:24,103:INFO:Creating metrics dataframe
2023-06-24 16:40:24,125:INFO:Initializing Decision Tree Regressor
2023-06-24 16:40:24,125:INFO:Total runtime is 0.7997312585512796 minutes
2023-06-24 16:40:24,131:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:24,132:INFO:Initializing create_model()
2023-06-24 16:40:24,132:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:24,132:INFO:Checking exceptions
2023-06-24 16:40:24,132:INFO:Importing libraries
2023-06-24 16:40:24,132:INFO:Copying training dataset
2023-06-24 16:40:24,142:INFO:Defining folds
2023-06-24 16:40:24,148:INFO:Declaring metric variables
2023-06-24 16:40:24,157:INFO:Importing untrained model
2023-06-24 16:40:24,163:INFO:Decision Tree Regressor Imported successfully
2023-06-24 16:40:24,176:INFO:Starting cross validation
2023-06-24 16:40:24,179:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:29,144:INFO:Calculating mean and std
2023-06-24 16:40:29,146:INFO:Creating metrics dataframe
2023-06-24 16:40:29,195:INFO:Uploading results into container
2023-06-24 16:40:29,197:INFO:Uploading model into container now
2023-06-24 16:40:29,198:INFO:_master_model_container: 12
2023-06-24 16:40:29,198:INFO:_display_container: 2
2023-06-24 16:40:29,199:INFO:DecisionTreeRegressor(random_state=123)
2023-06-24 16:40:29,199:INFO:create_model() successfully completed......................................
2023-06-24 16:40:29,439:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:29,440:INFO:Creating metrics dataframe
2023-06-24 16:40:29,471:INFO:Initializing Random Forest Regressor
2023-06-24 16:40:29,472:INFO:Total runtime is 0.8888288577397663 minutes
2023-06-24 16:40:29,483:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:29,483:INFO:Initializing create_model()
2023-06-24 16:40:29,483:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:29,484:INFO:Checking exceptions
2023-06-24 16:40:29,484:INFO:Importing libraries
2023-06-24 16:40:29,484:INFO:Copying training dataset
2023-06-24 16:40:29,497:INFO:Defining folds
2023-06-24 16:40:29,497:INFO:Declaring metric variables
2023-06-24 16:40:29,504:INFO:Importing untrained model
2023-06-24 16:40:29,518:INFO:Random Forest Regressor Imported successfully
2023-06-24 16:40:29,541:INFO:Starting cross validation
2023-06-24 16:40:29,544:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:42,421:INFO:Calculating mean and std
2023-06-24 16:40:42,433:INFO:Creating metrics dataframe
2023-06-24 16:40:42,481:INFO:Uploading results into container
2023-06-24 16:40:42,482:INFO:Uploading model into container now
2023-06-24 16:40:42,483:INFO:_master_model_container: 13
2023-06-24 16:40:42,483:INFO:_display_container: 2
2023-06-24 16:40:42,484:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-06-24 16:40:42,484:INFO:create_model() successfully completed......................................
2023-06-24 16:40:42,723:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:42,723:INFO:Creating metrics dataframe
2023-06-24 16:40:42,759:INFO:Initializing Extra Trees Regressor
2023-06-24 16:40:42,759:INFO:Total runtime is 1.1103004972139994 minutes
2023-06-24 16:40:42,773:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:42,773:INFO:Initializing create_model()
2023-06-24 16:40:42,773:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:42,773:INFO:Checking exceptions
2023-06-24 16:40:42,774:INFO:Importing libraries
2023-06-24 16:40:42,774:INFO:Copying training dataset
2023-06-24 16:40:42,793:INFO:Defining folds
2023-06-24 16:40:42,794:INFO:Declaring metric variables
2023-06-24 16:40:42,806:INFO:Importing untrained model
2023-06-24 16:40:42,817:INFO:Extra Trees Regressor Imported successfully
2023-06-24 16:40:42,845:INFO:Starting cross validation
2023-06-24 16:40:42,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:40:54,909:INFO:Calculating mean and std
2023-06-24 16:40:54,911:INFO:Creating metrics dataframe
2023-06-24 16:40:54,961:INFO:Uploading results into container
2023-06-24 16:40:54,963:INFO:Uploading model into container now
2023-06-24 16:40:54,964:INFO:_master_model_container: 14
2023-06-24 16:40:54,964:INFO:_display_container: 2
2023-06-24 16:40:54,965:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-06-24 16:40:54,965:INFO:create_model() successfully completed......................................
2023-06-24 16:40:55,154:INFO:SubProcess create_model() end ==================================
2023-06-24 16:40:55,154:INFO:Creating metrics dataframe
2023-06-24 16:40:55,176:INFO:Initializing AdaBoost Regressor
2023-06-24 16:40:55,176:INFO:Total runtime is 1.3172457416852315 minutes
2023-06-24 16:40:55,187:INFO:SubProcess create_model() called ==================================
2023-06-24 16:40:55,188:INFO:Initializing create_model()
2023-06-24 16:40:55,188:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:40:55,188:INFO:Checking exceptions
2023-06-24 16:40:55,188:INFO:Importing libraries
2023-06-24 16:40:55,188:INFO:Copying training dataset
2023-06-24 16:40:55,199:INFO:Defining folds
2023-06-24 16:40:55,199:INFO:Declaring metric variables
2023-06-24 16:40:55,210:INFO:Importing untrained model
2023-06-24 16:40:55,218:INFO:AdaBoost Regressor Imported successfully
2023-06-24 16:40:55,236:INFO:Starting cross validation
2023-06-24 16:40:55,240:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:41:01,987:INFO:Calculating mean and std
2023-06-24 16:41:02,004:INFO:Creating metrics dataframe
2023-06-24 16:41:02,047:INFO:Uploading results into container
2023-06-24 16:41:02,049:INFO:Uploading model into container now
2023-06-24 16:41:02,049:INFO:_master_model_container: 15
2023-06-24 16:41:02,049:INFO:_display_container: 2
2023-06-24 16:41:02,050:INFO:AdaBoostRegressor(random_state=123)
2023-06-24 16:41:02,050:INFO:create_model() successfully completed......................................
2023-06-24 16:41:02,238:INFO:SubProcess create_model() end ==================================
2023-06-24 16:41:02,239:INFO:Creating metrics dataframe
2023-06-24 16:41:02,259:INFO:Initializing Gradient Boosting Regressor
2023-06-24 16:41:02,260:INFO:Total runtime is 1.4353092908859253 minutes
2023-06-24 16:41:02,272:INFO:SubProcess create_model() called ==================================
2023-06-24 16:41:02,273:INFO:Initializing create_model()
2023-06-24 16:41:02,273:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:41:02,273:INFO:Checking exceptions
2023-06-24 16:41:02,273:INFO:Importing libraries
2023-06-24 16:41:02,273:INFO:Copying training dataset
2023-06-24 16:41:02,284:INFO:Defining folds
2023-06-24 16:41:02,284:INFO:Declaring metric variables
2023-06-24 16:41:02,293:INFO:Importing untrained model
2023-06-24 16:41:02,301:INFO:Gradient Boosting Regressor Imported successfully
2023-06-24 16:41:02,317:INFO:Starting cross validation
2023-06-24 16:41:02,320:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:41:08,857:INFO:Calculating mean and std
2023-06-24 16:41:08,860:INFO:Creating metrics dataframe
2023-06-24 16:41:08,933:INFO:Uploading results into container
2023-06-24 16:41:08,935:INFO:Uploading model into container now
2023-06-24 16:41:08,935:INFO:_master_model_container: 16
2023-06-24 16:41:08,935:INFO:_display_container: 2
2023-06-24 16:41:08,936:INFO:GradientBoostingRegressor(random_state=123)
2023-06-24 16:41:08,936:INFO:create_model() successfully completed......................................
2023-06-24 16:41:09,131:INFO:SubProcess create_model() end ==================================
2023-06-24 16:41:09,131:INFO:Creating metrics dataframe
2023-06-24 16:41:09,148:INFO:Initializing Extreme Gradient Boosting
2023-06-24 16:41:09,148:INFO:Total runtime is 1.550118088722229 minutes
2023-06-24 16:41:09,155:INFO:SubProcess create_model() called ==================================
2023-06-24 16:41:09,156:INFO:Initializing create_model()
2023-06-24 16:41:09,156:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:41:09,156:INFO:Checking exceptions
2023-06-24 16:41:09,156:INFO:Importing libraries
2023-06-24 16:41:09,156:INFO:Copying training dataset
2023-06-24 16:41:09,166:INFO:Defining folds
2023-06-24 16:41:09,166:INFO:Declaring metric variables
2023-06-24 16:41:09,174:INFO:Importing untrained model
2023-06-24 16:41:09,184:INFO:Extreme Gradient Boosting Imported successfully
2023-06-24 16:41:09,199:INFO:Starting cross validation
2023-06-24 16:41:09,207:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:41:17,970:INFO:Calculating mean and std
2023-06-24 16:41:17,974:INFO:Creating metrics dataframe
2023-06-24 16:41:18,022:INFO:Uploading results into container
2023-06-24 16:41:18,023:INFO:Uploading model into container now
2023-06-24 16:41:18,024:INFO:_master_model_container: 17
2023-06-24 16:41:18,024:INFO:_display_container: 2
2023-06-24 16:41:18,025:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=123, ...)
2023-06-24 16:41:18,025:INFO:create_model() successfully completed......................................
2023-06-24 16:41:18,211:INFO:SubProcess create_model() end ==================================
2023-06-24 16:41:18,211:INFO:Creating metrics dataframe
2023-06-24 16:41:18,230:INFO:Initializing Light Gradient Boosting Machine
2023-06-24 16:41:18,231:INFO:Total runtime is 1.70148823261261 minutes
2023-06-24 16:41:18,247:INFO:SubProcess create_model() called ==================================
2023-06-24 16:41:18,249:INFO:Initializing create_model()
2023-06-24 16:41:18,249:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:41:18,249:INFO:Checking exceptions
2023-06-24 16:41:18,249:INFO:Importing libraries
2023-06-24 16:41:18,250:INFO:Copying training dataset
2023-06-24 16:41:18,263:INFO:Defining folds
2023-06-24 16:41:18,263:INFO:Declaring metric variables
2023-06-24 16:41:18,271:INFO:Importing untrained model
2023-06-24 16:41:18,280:INFO:Light Gradient Boosting Machine Imported successfully
2023-06-24 16:41:18,298:INFO:Starting cross validation
2023-06-24 16:41:18,301:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:41:23,912:INFO:Calculating mean and std
2023-06-24 16:41:23,916:INFO:Creating metrics dataframe
2023-06-24 16:41:23,965:INFO:Uploading results into container
2023-06-24 16:41:23,966:INFO:Uploading model into container now
2023-06-24 16:41:23,967:INFO:_master_model_container: 18
2023-06-24 16:41:23,967:INFO:_display_container: 2
2023-06-24 16:41:23,967:INFO:LGBMRegressor(random_state=123)
2023-06-24 16:41:23,967:INFO:create_model() successfully completed......................................
2023-06-24 16:41:24,156:INFO:SubProcess create_model() end ==================================
2023-06-24 16:41:24,156:INFO:Creating metrics dataframe
2023-06-24 16:41:24,173:INFO:Initializing Dummy Regressor
2023-06-24 16:41:24,173:INFO:Total runtime is 1.800537399450938 minutes
2023-06-24 16:41:24,181:INFO:SubProcess create_model() called ==================================
2023-06-24 16:41:24,182:INFO:Initializing create_model()
2023-06-24 16:41:24,182:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30cc9d720>, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:41:24,182:INFO:Checking exceptions
2023-06-24 16:41:24,182:INFO:Importing libraries
2023-06-24 16:41:24,182:INFO:Copying training dataset
2023-06-24 16:41:24,192:INFO:Defining folds
2023-06-24 16:41:24,193:INFO:Declaring metric variables
2023-06-24 16:41:24,200:INFO:Importing untrained model
2023-06-24 16:41:24,207:INFO:Dummy Regressor Imported successfully
2023-06-24 16:41:24,221:INFO:Starting cross validation
2023-06-24 16:41:24,224:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:41:27,668:INFO:Calculating mean and std
2023-06-24 16:41:27,670:INFO:Creating metrics dataframe
2023-06-24 16:41:27,780:INFO:Uploading results into container
2023-06-24 16:41:27,782:INFO:Uploading model into container now
2023-06-24 16:41:27,783:INFO:_master_model_container: 19
2023-06-24 16:41:27,783:INFO:_display_container: 2
2023-06-24 16:41:27,783:INFO:DummyRegressor()
2023-06-24 16:41:27,783:INFO:create_model() successfully completed......................................
2023-06-24 16:41:28,025:INFO:SubProcess create_model() end ==================================
2023-06-24 16:41:28,025:INFO:Creating metrics dataframe
2023-06-24 16:41:28,081:INFO:Initializing create_model()
2023-06-24 16:41:28,081:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:41:28,081:INFO:Checking exceptions
2023-06-24 16:41:28,087:INFO:Importing libraries
2023-06-24 16:41:28,088:INFO:Copying training dataset
2023-06-24 16:41:28,097:INFO:Defining folds
2023-06-24 16:41:28,097:INFO:Declaring metric variables
2023-06-24 16:41:28,097:INFO:Importing untrained model
2023-06-24 16:41:28,097:INFO:Declaring custom model
2023-06-24 16:41:28,099:INFO:Gradient Boosting Regressor Imported successfully
2023-06-24 16:41:28,102:INFO:Cross validation set to False
2023-06-24 16:41:28,102:INFO:Fitting Model
2023-06-24 16:41:29,019:INFO:GradientBoostingRegressor(random_state=123)
2023-06-24 16:41:29,020:INFO:create_model() successfully completed......................................
2023-06-24 16:41:29,335:INFO:_master_model_container: 19
2023-06-24 16:41:29,335:INFO:_display_container: 2
2023-06-24 16:41:29,335:INFO:GradientBoostingRegressor(random_state=123)
2023-06-24 16:41:29,336:INFO:compare_models() successfully completed......................................
2023-06-24 16:41:34,135:INFO:Initializing create_model()
2023-06-24 16:41:34,136:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=gbr, fold=10, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:41:34,136:INFO:Checking exceptions
2023-06-24 16:41:34,175:INFO:Importing libraries
2023-06-24 16:41:34,176:INFO:Copying training dataset
2023-06-24 16:41:34,184:INFO:Defining folds
2023-06-24 16:41:34,187:INFO:Declaring metric variables
2023-06-24 16:41:34,197:INFO:Importing untrained model
2023-06-24 16:41:34,209:INFO:Gradient Boosting Regressor Imported successfully
2023-06-24 16:41:34,227:INFO:Starting cross validation
2023-06-24 16:41:34,231:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:41:37,927:INFO:Calculating mean and std
2023-06-24 16:41:37,940:INFO:Creating metrics dataframe
2023-06-24 16:41:37,958:INFO:Finalizing model
2023-06-24 16:41:38,218:INFO:Uploading results into container
2023-06-24 16:41:38,220:INFO:Uploading model into container now
2023-06-24 16:41:38,240:INFO:_master_model_container: 20
2023-06-24 16:41:38,240:INFO:_display_container: 3
2023-06-24 16:41:38,247:INFO:GradientBoostingRegressor(random_state=123)
2023-06-24 16:41:38,247:INFO:create_model() successfully completed......................................
2023-06-24 16:41:45,656:INFO:Initializing tune_model()
2023-06-24 16:41:45,657:INFO:tune_model(estimator=GradientBoostingRegressor(random_state=123), fold=10, round=4, n_iter=30, custom_grid={'learning_rate': [0.01, 0.02, 0.05], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8], 'subsample': [0.4, 0.5, 0.6, 0.7, 0.8], 'n_estimators': [100, 200.3, 400, 500, 600]}, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>)
2023-06-24 16:41:45,659:INFO:Checking exceptions
2023-06-24 16:41:45,702:INFO:Copying training dataset
2023-06-24 16:41:45,710:INFO:Checking base model
2023-06-24 16:41:45,710:INFO:Base model : Gradient Boosting Regressor
2023-06-24 16:41:45,718:INFO:Declaring metric variables
2023-06-24 16:41:45,728:INFO:Defining Hyperparameters
2023-06-24 16:41:45,937:INFO:custom_grid: {'actual_estimator__learning_rate': [0.01, 0.02, 0.05], 'actual_estimator__max_depth': [1, 2, 3, 4, 5, 6, 7, 8], 'actual_estimator__subsample': [0.4, 0.5, 0.6, 0.7, 0.8], 'actual_estimator__n_estimators': [100, 200.3, 400, 500, 600]}
2023-06-24 16:41:45,937:INFO:Tuning with n_jobs=-1
2023-06-24 16:41:45,937:INFO:Initializing RandomizedSearchCV
2023-06-24 16:41:55,057:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:41:55,302:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:41:56,530:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:41:57,218:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:41:59,780:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:09,282:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:42,596:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:43,076:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:44,387:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:45,083:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:46,768:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:46,918:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:47,875:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:47,980:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:49,794:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:49,806:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:50,855:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:50,940:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:52,668:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:52,789:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:53,733:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:53,932:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:55,824:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:56,359:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:42:57,864:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:42:58,571:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:01,083:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:01,461:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:02,145:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:02,757:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:10,600:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:12,231:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:14,454:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:21,158:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:21,999:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:22,250:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:23,080:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:24,936:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:25,712:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:26,394:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:27,485:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:31,092:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:31,737:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:32,197:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:32,866:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:34,640:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:35,362:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:35,734:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:36,447:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:38,245:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:38,912:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:39,339:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:40,061:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:49,773:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:43:54,737:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:43:55,743:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:22,127:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:22,371:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:23,187:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:23,473:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:28,461:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:29,096:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:30,806:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:31,058:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:35,184:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:35,750:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:36,303:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:40,185:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:40,717:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:41,757:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:47,079:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:47,736:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:48,164:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:48,820:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:57,717:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:44:59,587:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:44:59,898:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:03,197:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:04,304:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:08,719:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:11,983:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:13,950:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:16,266:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:21,978:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:23,035:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:25,107:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:26,236:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:28,336:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:30,672:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:32,169:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:34,224:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:36,424:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:38,476:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:42,016:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:43,864:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:50,082:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:50,258:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:51,179:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:51,348:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:55,475:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:55,570:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:45:56,664:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:45:56,726:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:03,411:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:46:03,556:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:46:04,494:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:04,707:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:08,741:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:46:09,056:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:46:09,846:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:10,151:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:16,170:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:46:16,605:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:46:17,820:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:18,021:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:38,842:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:43,018:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:44,172:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:57,422:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:46:57,965:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:00,564:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:47:04,485:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:11,292:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:47:16,970:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:47:17,654:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:19,247:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:47:20,322:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:29,523:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:31,191:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:33,285:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:47:44,785:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:50,285:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:57,625:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:47:58,008:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:47:59,100:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:02,447:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:03,282:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:05,540:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:08,621:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:09,222:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:10,282:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:11,973:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:13,752:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:14,801:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:15,738:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:16,925:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:19,536:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:21,655:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:23,501:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:24,855:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:25,648:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:25,928:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:26,778:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:28,093:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:28,942:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:29,253:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:30,037:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:31,491:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:33,015:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 1.31s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:33,076:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:35,152:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:37,419:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:38,187:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:38,525:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:39,330:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:40,704:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:41,505:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:41,790:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:42,624:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:49,242:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:51,191:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:55,576:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:56,626:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:48:57,559:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:48:58,634:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:00,642:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:01,672:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:04,347:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:07,962:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:12,421:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:13,758:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:18,629:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:19,131:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:28,630:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:33,082:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:34,349:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:34,868:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:51,540:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:51,613:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:49:52,777:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:53,331:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:49:59,510:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:05,079:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:09,791:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:23,334:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:24,081:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:24,885:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:25,413:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:29,762:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:30,602:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:31,063:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:34,947:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:35,567:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:36,784:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:37,621:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:43,282:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:43,895:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:44,415:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:48,732:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:49,319:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:49,832:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:56,381:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:56,582:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:50:57,476:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:50:57,725:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:06,894:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:51:07,947:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:51:08,387:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:09,589:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:13,046:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:51:23,296:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-24 16:51:24,149:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: 
50 fits failed out of a total of 300.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
50 fits failed with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "/usr/local/lib/python3.10/dist-packages/joblib/memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py", line 420, in fit
    self._validate_params()
  File "/usr/local/lib/python3.10/dist-packages/sklearn/base.py", line 600, in _validate_params
    validate_parameter_constraints(
  File "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py", line 97, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'n_estimators' parameter of GradientBoostingRegressor must be an int in the range [1, inf). Got 200.3 instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2023-06-24 16:51:24,174:WARNING:/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [           nan -7508.47170333 -4888.24563517            nan
 -5408.75978774 -4772.77404069 -4729.75901677 -4705.88623021
            nan -5422.12330595 -5112.77807834 -4727.48930429
 -4739.40479621 -4772.83177458            nan -5467.85017041
 -5105.00570421 -5179.34014183 -4729.65555082 -5417.84738241
 -4841.69701506 -4710.28969745 -4846.89042159            nan
 -5243.93883524 -4809.32376855 -4810.6836529  -4912.19052087
 -4950.24992857 -5382.90454497]
  warnings.warn(

2023-06-24 16:51:24,400:INFO:best_params: {'actual_estimator__subsample': 0.6, 'actual_estimator__n_estimators': 600, 'actual_estimator__max_depth': 2, 'actual_estimator__learning_rate': 0.01}
2023-06-24 16:51:24,407:INFO:Hyperparameter search completed
2023-06-24 16:51:24,407:INFO:SubProcess create_model() called ==================================
2023-06-24 16:51:24,409:INFO:Initializing create_model()
2023-06-24 16:51:24,409:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fd30d17c580>, model_only=True, return_train_score=False, kwargs={'subsample': 0.6, 'n_estimators': 600, 'max_depth': 2, 'learning_rate': 0.01})
2023-06-24 16:51:24,409:INFO:Checking exceptions
2023-06-24 16:51:24,409:INFO:Importing libraries
2023-06-24 16:51:24,409:INFO:Copying training dataset
2023-06-24 16:51:24,421:INFO:Defining folds
2023-06-24 16:51:24,421:INFO:Declaring metric variables
2023-06-24 16:51:24,432:INFO:Importing untrained model
2023-06-24 16:51:24,432:INFO:Declaring custom model
2023-06-24 16:51:24,441:INFO:Gradient Boosting Regressor Imported successfully
2023-06-24 16:51:24,455:INFO:Starting cross validation
2023-06-24 16:51:24,458:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:51:27,148:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:27,174:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:28,662:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:28,680:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:30,239:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:30,562:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:31,989:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:32,086:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:33,692:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:33,700:WARNING:/usr/local/lib/python3.10/dist-packages/pycaret/internal/pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-24 16:51:33,822:INFO:Calculating mean and std
2023-06-24 16:51:33,836:INFO:Creating metrics dataframe
2023-06-24 16:51:33,851:INFO:Finalizing model
2023-06-24 16:51:35,665:INFO:Uploading results into container
2023-06-24 16:51:35,667:INFO:Uploading model into container now
2023-06-24 16:51:35,669:INFO:_master_model_container: 21
2023-06-24 16:51:35,669:INFO:_display_container: 4
2023-06-24 16:51:35,670:INFO:GradientBoostingRegressor(learning_rate=0.01, max_depth=2, n_estimators=600,
                          random_state=123, subsample=0.6)
2023-06-24 16:51:35,670:INFO:create_model() successfully completed......................................
2023-06-24 16:51:35,966:INFO:SubProcess create_model() end ==================================
2023-06-24 16:51:35,967:INFO:choose_better activated
2023-06-24 16:51:35,972:INFO:SubProcess create_model() called ==================================
2023-06-24 16:51:35,974:INFO:Initializing create_model()
2023-06-24 16:51:35,974:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-24 16:51:35,974:INFO:Checking exceptions
2023-06-24 16:51:35,977:INFO:Importing libraries
2023-06-24 16:51:35,977:INFO:Copying training dataset
2023-06-24 16:51:35,985:INFO:Defining folds
2023-06-24 16:51:35,985:INFO:Declaring metric variables
2023-06-24 16:51:35,985:INFO:Importing untrained model
2023-06-24 16:51:35,985:INFO:Declaring custom model
2023-06-24 16:51:35,986:INFO:Gradient Boosting Regressor Imported successfully
2023-06-24 16:51:35,986:INFO:Starting cross validation
2023-06-24 16:51:35,988:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-24 16:51:39,929:INFO:Calculating mean and std
2023-06-24 16:51:39,930:INFO:Creating metrics dataframe
2023-06-24 16:51:39,934:INFO:Finalizing model
2023-06-24 16:51:40,420:INFO:Uploading results into container
2023-06-24 16:51:40,421:INFO:Uploading model into container now
2023-06-24 16:51:40,422:INFO:_master_model_container: 22
2023-06-24 16:51:40,423:INFO:_display_container: 5
2023-06-24 16:51:40,423:INFO:GradientBoostingRegressor(random_state=123)
2023-06-24 16:51:40,423:INFO:create_model() successfully completed......................................
2023-06-24 16:51:40,678:INFO:SubProcess create_model() end ==================================
2023-06-24 16:51:40,679:INFO:GradientBoostingRegressor(random_state=123) result for RMSE is 4956.9417
2023-06-24 16:51:40,680:INFO:GradientBoostingRegressor(learning_rate=0.01, max_depth=2, n_estimators=600,
                          random_state=123, subsample=0.6) result for RMSE is 4705.8862
2023-06-24 16:51:40,681:INFO:GradientBoostingRegressor(learning_rate=0.01, max_depth=2, n_estimators=600,
                          random_state=123, subsample=0.6) is best model
2023-06-24 16:51:40,681:INFO:choose_better completed
2023-06-24 16:51:40,703:INFO:_master_model_container: 22
2023-06-24 16:51:40,704:INFO:_display_container: 4
2023-06-24 16:51:40,705:INFO:GradientBoostingRegressor(learning_rate=0.01, max_depth=2, n_estimators=600,
                          random_state=123, subsample=0.6)
2023-06-24 16:51:40,705:INFO:tune_model() successfully completed......................................
2023-06-24 16:51:47,012:INFO:Initializing predict_model()
2023-06-24 16:51:47,013:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fd30d599ea0>)
2023-06-24 16:51:47,013:INFO:Checking exceptions
2023-06-24 16:51:47,013:INFO:Preloading libraries
2023-06-24 16:51:50,750:INFO:Initializing evaluate_model()
2023-06-24 16:51:50,751:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2023-06-24 16:51:50,789:INFO:Initializing plot_model()
2023-06-24 16:51:50,789:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, system=True)
2023-06-24 16:51:50,789:INFO:Checking exceptions
2023-06-24 16:51:50,796:INFO:Preloading libraries
2023-06-24 16:51:50,807:INFO:Copying training dataset
2023-06-24 16:51:50,807:INFO:Plot type: pipeline
2023-06-24 16:51:51,156:INFO:Visual Rendered Successfully
2023-06-24 16:51:51,367:INFO:plot_model() successfully completed......................................
2023-06-24 16:51:54,024:INFO:Initializing finalize_model()
2023-06-24 16:51:54,024:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-06-24 16:51:54,025:INFO:Finalizing GradientBoostingRegressor(random_state=123)
2023-06-24 16:51:54,034:INFO:Initializing create_model()
2023-06-24 16:51:54,035:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fd30de9cd30>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-06-24 16:51:54,035:INFO:Checking exceptions
2023-06-24 16:51:54,038:INFO:Importing libraries
2023-06-24 16:51:54,038:INFO:Copying training dataset
2023-06-24 16:51:54,039:INFO:Defining folds
2023-06-24 16:51:54,044:INFO:Declaring metric variables
2023-06-24 16:51:54,044:INFO:Importing untrained model
2023-06-24 16:51:54,044:INFO:Declaring custom model
2023-06-24 16:51:54,046:INFO:Gradient Boosting Regressor Imported successfully
2023-06-24 16:51:54,048:INFO:Cross validation set to False
2023-06-24 16:51:54,049:INFO:Fitting Model
2023-06-24 16:51:55,356:INFO:Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=123))])
2023-06-24 16:51:55,357:INFO:create_model() successfully completed......................................
2023-06-24 16:51:55,605:INFO:_master_model_container: 22
2023-06-24 16:51:55,606:INFO:_display_container: 5
2023-06-24 16:51:55,692:INFO:Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=123))])
2023-06-24 16:51:55,693:INFO:finalize_model() successfully completed......................................
2023-06-24 16:51:55,971:INFO:Initializing save_model()
2023-06-24 16:51:55,971:INFO:save_model(model=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=123))]), model_name=insurance_model, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-06-24 16:51:55,971:INFO:Adding model into prep_pipe
2023-06-24 16:51:55,971:WARNING:Only Model saved as it was a pipeline.
2023-06-24 16:51:55,984:INFO:insurance_model.pkl saved in current working directory
2023-06-24 16:51:56,034:INFO:Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWrapper(include=['sex', 'smoker...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=123))])
2023-06-24 16:51:56,034:INFO:save_model() successfully completed......................................
2023-06-24 23:33:45,788:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 23:33:45,804:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 23:33:45,804:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 23:33:45,804:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-24 23:33:56,232:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-24 23:34:17,302:INFO:Initializing load_model()
2023-06-24 23:34:17,302:INFO:Initializing load_model()
2023-06-24 23:34:17,302:INFO:load_model(model_name=insurance_model, platform=None, authentication=None, verbose=True)
2023-06-24 23:34:17,303:INFO:load_model(model_name=insurance_model, platform=None, authentication=None, verbose=True)
2023-06-24 23:34:17,782:WARNING:C:\Users\Ravi\AppData\Local\Programs\Python\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '63.2.0', 'pycaret': '3.0.2', 'IPython': '8.14.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.5', 'pandas': '1.5.3', 'jinja2': '3.1.2', 'scipy': '1.10.1', 'joblib': '1.2.0', 'sklearn': '1.2.2', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.1', 'requests': '2.31.0', 'matplotlib': '3.7.1', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.15.0', 'kaleido': '0.2.1', 'statsmodels': '0.14.0', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.5'}, 'python': {'version': '3.10.7', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '67.7.2', 'pycaret': '3.0.2', 'IPython': '7.34.0', 'ipywidgets': '7.7.1', 'tqdm': '4.65.0', 'numpy': '1.22.4', 'pandas': '1.5.3', 'jinja2': '3.1.2', 'scipy': '1.10.1', 'joblib': '1.2.0', 'sklearn': '1.2.2', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.56.4', 'requests': '2.27.1', 'matplotlib': '3.7.1', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.5'}, 'python': {'version': '3.10.12', 'machine': 'x86_64'}}
  warnings.warn(

